<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/"><link rel="shortcut icon" href="favicon.ico"><link rel="stylesheet" href="jemdoc.css" type="text/css"><title>Wenbin Li's Homepage (Nanjing University)</title><style type="text/css"></style></head><body><div id="layout-content"><div id="toptitle"><h1>Wenbin Li at Nanjing University</h1></div><table class="imgtable"><tbody><tr><td><a href="lwb4.jpg"><img src="lwb4.jpg" alt="lwb4.jpg" width="220px" height="270px"></a>&nbsp;</td><td align="left"><p><b><font size="+1" face="Times New Roman">Wenbin Li</font> <font size="+1" face="华文楷体">(李文斌)</font></b><br><font size="+1" face="Times New Roman">Associate Professor</font> <br><br><a href="https://cs.nju.edu.cn/rl/index_eng.htm">Reasoning and Learning Research Group</a><br><a href="https://is.nju.edu.cn/main.htm">School of Intelligence Science and Technology</a> <br><a href="http://keysoftlab.nju.edu.cn/site/ndjsjx/">State Key Laboratory for Novel Software Technology</a> <br><a href="http://www.nju.edu.cn/">Nanjing University, Suzhou Campus</a><br><br>Email: <a href="mailto:liwenbin@nju.edu.cn">liwenbin@nju.edu.cn</a>;  <a href="mailto:liwenbin.nju@gmail.com">liwenbin.nju@gmail.com</a><br><br><!-- Office: Room 1019, Computer Science Building, Xianlin Campus of Nanjing University, Nanjing 210023, China</p> --><a href="https://scholar.google.com/citations?user=K-kC4yYAAAAJ&hl=zh-CN&authuser=1"><span style="color:purple">Google Scholar</span></a> | <a href="https://github.com/WenbinLee"><span style="color:purple">Github</span></a> |<a href="https://github.com/RL-VIG"><span style="color:purple">Github-VIG</span></a></td></tbody></table><h2><font face="Times New Roman">Biography</font></h2><p><font face="Times New Roman">Currently, I am a Tenure-Track Associate Professor of <a href="https://is.nju.edu.cn/main.htm" target="_blank">School of Intelligence Science and Technology</a> at <a href="http://www.nju.edu.cn/" target="_blank">Nanjing University</a> and a member of <a href="https://cs.nju.edu.cn/rl/" target="_blank">Reasoning and Learning Research Group</a>, led by professor <a href="https://cs.nju.edu.cn/gaoyang">Yang Gao</a>.<br><span class="norm"><br class="style1"></span>I received my Ph.D. degree in <a href="http://cs.nju.edu.cn/" target="_blank">Department of Computer Science and Technology</a> in December 2019 from <a href="https://www.nju.edu.cn/EN/">Nanjing University</a>.<span class="norm"><br class="style1"></span>I received my B.Sc. degree in <a href="http://cs.cumt.edu.cn/">School of Computer Science and Technology</a> in June 2013 from <a href="http://http://www.cumt.edu.cn//">China University of Mining and Technology</a>.<span class="norm"><br class="style1"></span>I visited the <a href="http://www.cs.rochester.edu/u/jluo/#VISTA">VIStA</a> group from December 2017 to June 2019 at the <a href="https://www.rochester.edu">University of Rochester</a>, under the supervision of Prof. <a href="http://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>.</font></p><h2><font face="Times New Roman">News</font></h2><ul><li><p><b><font face="Times New Roman">[09/2024]</b>: <a>课题组长期招收博后、博士研究生和硕士研究生, 欢迎与我们联系!</font></a></font></p></li><li><p><b><font face="Times New Roman">[05/2024]</b>: <b>  <a href="https://github.com/RL-VIG/LibContinual.git">A Comprehensive Library for Continual Learning: LibContinual</b></a> is released.</font></p></li><li><p><b><font face="Times New Roman">[05/2024]</b>: One paper on “Imperfect Information Games” is accepted to <a href="https://icml.cc/">ICML 2024</a>.</font></p></li><li><p><b><font face="Times New Roman">[04/2024]</b>: Two papers on “Activity Recognition” and “MARL” are accepted to <a href="hhttps://ijcai24.org/">IJCAI 2024</a>.</font></p></li><li><p><b><font face="Times New Roman">[02/2024]</b>: One paper on “Dataset Distillation” is accepted to <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.</font></p></li><li><p><b><font face="Times New Roman">[01/2024]</b>: One paper on “Neural Radiance Fields (NeRF)” is accepted to <a href="https://iclr.cc/">ICLR 2024</a>.</font></p></li><li><p><b><font face="Times New Roman">[02/2024]</b>: One paper on “Adversarial Training” is accepted to <a href="https://www.sciencedirect.com/science/article/pii/S0893608024001485">Neural Networks 2024</a>.</font></p></li><li><p><b><font face="Times New Roman">[01/2024]</b>: One paper on “Video Style Transfer” is accepted to <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding">CVIU 2024</a>.</font></p></li><li><p><b><font face="Times New Roman">[12/2023]</b>: One paper on “Multi-Agent Reinforcement Learning” is accepted to <a href="https://aaai.org/aaai-conference/">AAAI 2024</a>.</font></p></li><li><p><b><font face="Times New Roman">[11/2023]</b>: One paper on “Multi-Agent Reinforcement Learning” is accepted to <a href="https://ieeexplore.ieee.org/document/10324374">TCYB 2023</a>.</font></p></li><li><p><b><font face="Times New Roman">[09/2023]</b>: One paper on “Subgame solving” is accepted to <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Conference">NeurIPS 2023</a>.</font></p></li><li><p><b><font face="Times New Roman">[09/2023]</b>: One paper on “Few-shot learning” is accepted to <a href="https://ieeexplore.ieee.org/document/9916072">TPAMI 2023</a>.</font></p></li><!-- <li><p><b><font face="Times New Roman">[09/2023]</b>: One paper on “Meta Learning” is accepted to <a href="https://dl.acm.org/doi/10.1145/3618365">SIGGRAPH Asia 2023</a>.</font></p></li> --><li><p><b><font face="Times New Roman">[07/2023]</b>: One paper on “Neural Radiance Fields (NeRF)” is accepted to <a href="https://www.acmmm2023.org/">ACM MM 2023</a>.</font></p></li><li><p><b><font face="Times New Roman">[07/2023]</b>: One paper on “Out-of-distribution generalization” is accepted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>.</font></p></li><li><p><b><font face="Times New Roman">[05/2023]</b>: To serve as Area Chair of Winter Conference on Applications of Computer Vision <a href="https://wacv2024.thecvf.com/">WACV 2024</a>.</font></p></li><li><p><b><font face="Times New Roman">[05/2023]</b>: One paper on “Self-supervised learning” is accepted to <a href="https://openreview.net/group?id=TMLR/">TMLR 2023</a>.</font></p></li><li><p><b><font face="Times New Roman">[05/2023]</b>: One paper on “Few-shot learning” is accepted to <a href="https://www.sciencedirect.com/science/article/pii/S0031320323004004?dgcid=author">Pattern Recognition 2023</a>.</font></p></li><li><p><b><font face="Times New Roman">[03/2023]</b>: One paper on “Novel class discovery” is accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.</font></p></li><li><p><b><font face="Times New Roman">[11/2022]</b>: One paper on “Multi-agent reinforcement learning” is accepted to <a href="https://aaai.org/Conferences/AAAI-23/">AAAI 2023</a>.</font></p></li><li><p><b><font face="Times New Roman">[10/2022]</b>: One paper on “Few-shot learning” is accepted to <a href="https://ieeexplore.ieee.org/document/9916072">TPAMI 2023</a>.</font></p></li><li><p><b><font face="Times New Roman">[10/2022]</b>: One paper on “Online metric learning” is accepted to <a href="https://ieeexplore.ieee.org/document/9928295">TNNLS 2022</a>.</font></p></li><li><p><b><font face="Times New Roman">[09/2022]</b>: Serve as Publication Chair of 16th Asian Conference on Computer Vision <a href="https://www.accv2022.org/en/">ACCV 2022</a>.</font></p></li><li><p><b><font face="Times New Roman">[07/2022]</b>: One paper on “Few-shot learning” is accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.</font></p></li><li><p><b><font face="Times New Roman">[05/2022]</b>: One paper on “Caricature generation” is accepted to <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765839">TIP 2022</a>.</font></p></li><li><p><b><font face="Times New Roman">[05/2022]</b>: One paper on “Online learning” is accepted to <a href="https://ieeexplore.ieee.org/document/9791447">TNNLS 2022</a>.</font></p></li></ul><h2><font face="Times New Roman">Research Interests</font></h2><p> <font face="Times New Roman">My research interests include Brain-inspired Artificial Intelligence, Machine Learning and Collaborative optimization of software and hardware,</font></p><ul><li><p><font face="Times New Roman">Few-shot Learning</font></p></li><li><p><font face="Times New Roman">Continual Learning</font></p></li><li><p><font face="Times New Roman">Self-supervised Learning</font></p></li><li><p><font face="Times New Roman">Machine Learning Compilation</font></p></li><li><p><font face="Times New Roman">Compute in Memory</font></p></li></ul><h2><font face="Times New Roman">Preprints</font></h2></ol><table class="imgtable"><tbody><tr><td><img src="./Paper/ONNXPruner.png" alt="WSFG" width="220px" height="100px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Dongdong Ren, <b>Wenbin Li</b>, Tianyu Ding, Lei Wang, Qi Fan, Jing Huo, Hongbing Pan and Yang Gao.<br> <a href="https://arxiv.org/pdf/2404.08016">ONNXPruner: ONNX-Based General Model Pruning Adapter.</a><br> In: <em>arXiv preprint arXiv:2404.08016</em>, 2024.<br> </font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/UniCLR.png" alt="WSFG" width="220px" height="100px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"><b>Wenbin Li</b>, Meihao Kong, Xuesong Yang, Lei Wang, Jing Huo, Yang Gao and Jiebo Luo.<br> <a href="https://arxiv.org/abs/2211.14516">A Unified Framework for Contrastive Learning from a Perspective of Affinity Matrix.</a><br> In: <em>arXiv preprint arXiv:2211.14516</em>, 2022.<br> </font></p></li></ul></td></tr></tbody></table></ol><h2><font face="Times New Roman">Publications (Selected)</font></h2><h3><font face="Times New Roman">Journal Articles:</font></h3></ol><table class="imgtable"><tbody><tr><td><img src="./Paper/AELF.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Changbin Shao, <b>Wenbin Li*</b>, Jing Huo, Zhenhua Feng, Yang Gao.<br><a href="https://www.sciencedirect.com/science/article/pii/S0893608024001485"> Attention-based Investigation and Solution to the Trade-off Issue of Adversarial Training</a>. <br>In: <em> <b>Neural Networks</b></em>, 2024. <br> (Impact Factor: 7.8) <br>[<a href="https://www.sciencedirect.com/science/article/pii/S0893608024001485">Paper</a>] [<a href="https://github.com/CV-SCB0/AELF.git">Code</a>]</font></p> </li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/DIST.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Jing Huo, Meihao Kong, <b>Wenbin Li</b>, Jing Wu, Yu-Kun Lai, Yang Gao.<br><a href="https://www.sciencedirect.com/science/article/pii/S1077314224000286"> Towards Efficient Image and Video Style Transfer via Distillation and Learnable Feature Transformation</a>. <br>In: <em>Computer Vision and Image Understanding <b>(CVIU)</b></em>, 2024. <br> (Impact Factor: 4.5) <br>[<a href="https://www.sciencedirect.com/science/article/pii/S1077314224000286">Paper</a>] [<a href="https://cs.nju.edu.cn/huojing/DistDemo.htm">Demo</a>]</font></p> </li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/Libfewshot.png" alt="WSFG" width="220px" height="100px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"><b>Wenbin Li</b>, Ziyi Wang, Xuesong Yang, Chuanqi Dong, Pinzhuo Tian, Tiexin Qin, Jing Huo, Yinghuan Shi, Lei Wang, Yang Gao and Jiebo Luo.<br> <a href="https://arxiv.org/abs/2109.04898">LibFewShot: A Comprehensive Library for Few-shot Learning.</a><br>  In: <em>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b></em>, 2023.<br>   (Impact Factor: 23.6) <br> [<a href="./LibFewShot.pdf" download="LibFewShot.pdf">Paper</a>] [<a href="https://github.com/RL-VIG/LibFewShot.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/MDAT.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"><b>Wenbin Li</b>, Lei Wang, Xingxing Zhang, Lei Qi, Jing Huo, Yang Gao, Jiebo Luo.<br> <a href="https://ieeexplore.ieee.org/document/9916072">Defensive Few-shot Learning.</a><br> In: <em>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b></em>, 2023.<br>   (Impact Factor: 23.6) <br>[<a href="./TPAMI_DFSL.pdf" download="TPAMI_DFSL.pdf">Paper</a>] [<a href="https://github.com/WenbinLee/DefensiveFSL.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/ROMA.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td  align="left"><ul><li><p><font face="Times New Roman"><b>Wenbin Li</b>, Xuesong Yang, Meihao Kong, Lei Wang, Jing Huo, Yang Gao and Jiebo Luo.<br> <a href="https://openreview.net/pdf?id=MR4glug5GU">Trip-ROMA: Self-Supervised Learning with Triplets and Random Mappings.</a><br> In: <em>Transactions on Machine Learning Research <b>(TMLR)</b></em>, 2023.<br>  [<a href="./Trip_ROMA.pdf" download="Trip_ROMA.pdf">Paper</a>] [<a href="https://github.com/WenbinLee/Trip-ROMA.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/WTOE.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Shaokang Dong, Hangyu Mao, Shangdong Yang, Shengyu Zhu, <b>Wenbin Li*</b>, Jianye Hao, Yang Gao<br><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10324374"> WToE: Learning When to Explore in Multiagent Reinforcement Learning</a>. <br>In: <em>IEEE Transactions on Cybernetics <b>(TCYB)</b></em>, 2023. <br> (Impact Factor: 11.8) <br><!-- [<a href="./PR23.pdf" download="PR23.pdf">Paper</a>] [<a href="https://github.com/onlyyao/GLFA-SOLF">Code</a>] --></font></p> </li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/GLFA.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Boyao Shi, <b>Wenbin Li*</b>, Jing Huo, Pengfei Zhu, Lei Wang, Yang Gao. <br><a href="https://www.sciencedirect.com/science/article/pii/S0031320323004004?dgcid=author"> Global- and Local-aware Feature Augmentation with Semantic Orthogonality for Few-shot Image Classification</a>. <br>In: <b><i>Pattern Recognition</i></b>, 2023.<br> (Impact Factor: 8.0) <br>[<a href="./PR23.pdf" download="PR23.pdf">Paper</a>] [<a href="https://github.com/onlyyao/GLFA-SOLF">Code</a>]</font></p> </li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/CAST.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"> Jing Huo, Xiangde Liu, <b>Wenbin Li</b>, Yang Gao, Hujun Yin, Jiebo Luo.<br> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765839">CAST: Learning Both Geometric and Texture Style Transfers for Effective Caricature Generation.</a><br> In: <em>IEEE Transactions on Image Processing <b>(TIP)</b></em>, 2022. <br>   (Impact Factor: 10.6) <br>  [<a href="./CAST.pdf" download="CAST.pdf">Paper</a>] </font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/ODML.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"><b>Wenbin Li</b>, Yanfang Liu, Jing Huo, Yinghuan Shi, Yang Gao, Lei Wang, Jiebo Luo.<br>   <a href="https://ieeexplore.ieee.org/document/9928295/authors#authors">A Multilayer Framework for Online Metric Learning.</a><br> In: <em>IEEE Transactions on Neural Networks and Learning Systems <b>(TNNLS)</b></em>, 2022. <br>  (Impact Factor: 10.4) <br></font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/PAA.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Yanfang Liu, Xiaocong Fan, <b>Wenbin Li</b>, Yang Gao.<br> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9791447">Online Passive-Aggressive Active Learning for Trapezoidal Data Streams.</a><br> In: <em>IEEE Transactions on Neural Networks and Learning Systems <b>(TNNLS)</b></em>, 2022. <br>  (Impact Factor: 10.4) <br>  [<a href="./PAA.pdf" download="PAA.pdf">Paper</a>] </font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/MetaReg.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Pinzhuo Tian, <b>Wenbin Li</b>, Yang Gao.<br> <a href="https://ieeexplore.ieee.org/document/9449618">Consistent Meta-Regularization for Better Meta-Knowledge in Few-Shot Learning.</a><br> In: <em>IEEE Transactions on Neural Networks and Learning Systems <b>(TNNLS)</b></em>, 2021.<br>  (Impact Factor: 10.4) <br> [<a href="./TNNLS_Tian.pdf" download="TNNLS_Tian.pdf">Paper</a>] </font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/CariMe.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Zheng Gu, Chuanqi Dong, Jing Huo, <b>Wenbin Li</b>, Yang Gao.<br> <a href="https://ieeexplore.ieee.org/document/9454341">CariMe: Unpaired Caricature Generation with Multiple Exaggerations.</a><br> In: <em>IEEE Transactions on Multimedia <b>(TMM)</b></em>, 2021.<br>  (Impact Factor: 7.3) <br> [<a href="./TMM_GuZheng.pdf" download="TMM_GuZheng.pdf">Paper</a>] [<a href="https://github.com/edward3862/CariMe-pytorch.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/CariGAN.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"><b>Wenbin Li</b>, Wei Xiong, Haofu Liao, Jing Huo, Yang Gao, Jiebo Luo.<br> <a href="https://www.sciencedirect.com/science/article/pii/S0893608020303002">CariGAN: Caricature Generation through Weakly Paired Adversarial Learning.</a><br> In: <em><b>Neural Networks</b></em>, 2020.<br>  (Impact Factor: 7.8) <br> [<a href="./arXiv_Cari.pdf" download="arXiv_Cari.pdf">Paper</a>] </font></p></li></ul></td></tr></tbody></table><br><h3><font face="Times New Roman">Conference Articles:</font></h3><!-- </ol> --><table class="imgtable"><tbody><tr><td><img src="./Paper/OX-Search.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Zhenxing Ge, Zheng Xu, Tianyu Ding, Linjian Meng, Bo An, <b>Wenbin Li</b>, Yang Gao.<br> <a href="https://icml.cc/">Safe and Robust Subgame Exploitation in Imperfect Information Games.</a><br>  In: <em>International Conference on Machine Learning <b>(ICML)</b></em>, 2024.<br>    (Acceptance Rate: 2609/9473=27.5%) <br>  <!-- [<a href="http://arxiv.org/abs/2404.00563">arXiv</a>] [<a href="https://github.com/VincenDen/IID">Code</a>] --></font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/STAR.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Chao Li, Yujing Hu, Shangdong Yang, Tangjie Lv, Changjie Fan, <b>Wenbin Li*</b>, Chongjie Zhang, Yang Gao*.<br> <a href="https://ijcai24.org/">STAR: Spatio-Temporal State Compression for Multi-Agent Tasks with Rich Observations.</a><br>  In: <em>International Joint Conference on Artificial Intelligence <b>(IJCAI)</b></em>, 2024.<br>    <!-- (Acceptance Rate: 2719/11532=23.6%) <br> -->  <!-- [<a href="http://arxiv.org/abs/2404.00563">arXiv</a>] [<a href="https://github.com/VincenDen/IID">Code</a>] --></font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/DTS-TPT.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Rui Yan, Hongyu Qu, Xiangbo Shu, <b>Wenbin Li*</b>, Jinghui Tang, Tieniu Tan.<br> <a href="https://ijcai24.org/">DTS-TPT: Dual Temporal-Sync Test-time Prompt Tuning for Zero-shot Activity Recognition.</a><br>  In: <em>International Joint Conference on Artificial Intelligence <b>(IJCAI)</b></em>, 2024.<br>    <!-- (Acceptance Rate: 2719/11532=23.6%) <br> -->  <!-- [<a href="http://arxiv.org/abs/2404.00563">arXiv</a>] [<a href="https://github.com/VincenDen/IID">Code</a>] --></font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/DD.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Wenxiao Deng, <b>Wenbin Li*</b>, Tianyu Ding, Lei Wang, Hongguang Zhang, <br> Kuihua Huang, Jing Huo, and Yang Gao.<br> <a href="https://arxiv.org/pdf/2404.00563.pdf">Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation.</a><br>  In: <em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2024.<br>    (Acceptance Rate: 2719/11532=23.6%) <br>  [<a href="http://arxiv.org/abs/2404.00563">arXiv</a>] [<a href="https://github.com/VincenDen/IID">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/InsertNeRF.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Yanqi Bao, Tianyu Ding, Jing Huo, <b>Wenbin Li</b>, Yuxin Li, and Yang Gao.<br> <a href="https://openreview.net/forum?id=aHmNpLlUlb&noteId=7DmKGOB0Hd">InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules.</a><br>  In: <em>International Conference on Learning Representations  <b>(ICLR)</b></em>, 2024.<br>    (Acceptance Rate: 2251/7262=31%) <br>  [<a href="./InsertNeRF_ICLR2024.pdf" download="InsertNeRF_ICLR2024.pdf">Paper</a>] [<a href="https://github.com/bbbbby-99/InsertNeRF">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/OVI.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Chao Li , Yupeng Zhang, Jianqi Wang, Yujing Hu, Shaokang Dong, <b>Wenbin Li</b>, <br> Tangjie Lv, Changjie Fan, Yang Gao.<br> <a href="https://openreview.net/forum?id=8HzOyg1ngp">Optimistic Value Instructors for Cooperative Multi-Agent Reinforcement Learning.</a><br>  In: <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2024.<br>    (Acceptance Rate: 2342/9862=23.75%) <br></font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/subgame.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Zhenxing Ge, Zheng Xu, Tianyu Ding, <b>Wenbin Li*</b>, Yang Gao*.<br> <a href="https://openreview.net/forum?id=8HzOyg1ngp">Efficient Subgame Refinement for Extensive-form Games.</a><br>  In: <em>Conference on Neural Information Processing Systems <b>(NeurIPS)</b></em>, 2023.<br>   (Acceptance Rate: 3222/12343=26.1%) <br></font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/Nerf.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Yanqi Bao, Yuxin Li, Jing Huo, Tianyu Ding, Xinyue Liang, <b>Wenbin Li</b>, Yang Gao.<br> <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3613769">Where and How: Mitigating Confusion in Neural Radiance Fields from Sparse Inputs.</a><br>  In: <em>ACM Conference on Multimedia <b>(ACM MM)</b></em>, 2023.<br>   (Acceptance Rate: 902/3072=29.3%) <br> <!-- [<a href="./NCD_IIC.pdf" download="NCD_IIC.pdf">Paper</a>] [<a href="https://github.com/FanZhichen/NCD-IIC.git">Code</a>] --></font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/MAP.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Min Zhang, Junkun Yuan, Yue He, <b>Wenbin Li</b>, Zhengyu Chen, Kun Kuang.<br> <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MAP_Towards_Balanced_Generalization_of_IID_and_OOD_through_Model-Agnostic_ICCV_2023_paper.pdf">MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters.</a><br>  In: <em>International Conference on Computer Vision <b>(ICCV)</b></em>, 2023. <b>[Oral]</b><br> <!-- [<a href="./NCD_IIC.pdf" download="NCD_IIC.pdf">Paper</a>] [<a href="https://github.com/FanZhichen/NCD-IIC.git">Code</a>] --></font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/NCD.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"><b>Wenbin Li</b>, Zhichen Fan, Jing Huo, Yang Gao.<br> <a href="https://arxiv.org/pdf/2210.03591.pdf">Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery.</a><br>  In: <em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2023.<br>  (Acceptance Rate: 2360/9155=25.78%) <br> [<a href="./NCD_IIC.pdf" download="NCD_IIC.pdf">Paper</a>] [<a href="https://github.com/FanZhichen/NCD-IIC.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/MAPPG.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Wubing Chen, <b>Wenbin Li*</b>, Xiao Liu, Shangdong Yang, Yang Gao*.<br> <a href="https://arxiv.org/pdf/2210.05367.pdf">Learning Explicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning <br>via Polarization Policy Gradient.</a><br>  In: <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2023.<br>  (Acceptance Rate: 1721/8777=19.6%) <br>  [<a href="./MAPPG_AAAI2023.pdf" download="MAPPG_AAAI2023.pdf">Paper</a>] [<a href="https://github.com/code-cultivater/MAPPG">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/HTS.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Min Zhang, Siteng Huang, <b>Wenbin Li</b>, Donglin Wang.<br> <a href="https://arxiv.org/pdf/2207.06989.pdf">Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation.</a><br> In: <em>European Conference on Computer Vision <b>(ECCV)</b></em>, 2022. <br>  (Acceptance Rate: 1650/5803=28%) <br>  [<a href="./HTS_ECCV2022.pdf" download="HTS_ECCV2022.pdf">Paper</a>] [<a href="https://github.com/remiMZ/HTS-ECCV22.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/LoFGAN.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Zheng Gu<sup>#</sup>, <b>Wenbin Li<sup>#</sup></b>, Jing Huo, Lei Wang, Yang Gao.<br> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_LoFGAN_Fusing_Local_Representations_for_Few-Shot_Image_Generation_ICCV_2021_paper.pdf">LoFGAN: Fusing Local Representations for Few-shot Image Generation.</a><br> In: <em>International Conference on Computer Vision <b>(ICCV)</b></em>, 2021. <br>   (Acceptance Rate: 1617/6236=25.9%) [# Equal contribution]<br>  [<a href="./LoFGAN.pdf" download="LoFGAN.pdf">Paper</a>] [<a href="https://github.com/edward3862/LoFGAN-pytorch.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/MAST.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Jing Huo, Shiyin Jin, <b>Wenbin Li</b>, Jing Wu, Yu-Kun Lai, Yinghuan Shi, Yang Gao.<br> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Huo_Manifold_Alignment_for_Semantically_Aligned_Style_Transfer_ICCV_2021_paper.pdf">Manifold Alignment for Semantically Aligned Style Transfer.</a><br> In: <em>International Conference on Computer Vision <b>(ICCV)</b></em>, 2021. <b>[Oral]</b><br>   (Acceptance Rate: 1617/6236=25.9%; Oral: 210/6236=3%) <br> [<a href="./MAST.pdf" download="MAST.pdf">Paper</a>] [<a href="https://github.com/NJUHuoJing/MAST.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/ADM.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"><b>Wenbin Li</b>, Lei Wang, Jing Huo, Yinghuan Shi, Yang Gao, Jiebo Luo.<br> <a href="https://www.ijcai.org/Proceedings/2020/0409.pdf">Asymmetric Distribution Measure for Few-shot Learning.</a><br> In: <em>International Joint Conference on Artificial Intelligence <b>(IJCAI)</b></em>, 2020.<br>  (Acceptance Rate: 592/4717=12.6%) <br> [<a href="./ADM.pdf" download="ADM.pdf">Paper</a>] [<a href="https://github.com/WenbinLee/ADM.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/ATL.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Chuanqi Dong, <b>Wenbin Li</b>, Jing Huo, Zheng Gu, Yang Gao.<br> <a href="https://www.ijcai.org/Proceedings/2020/0100.pdf">Learning Task-aware Local Representations for Few-shot Learning.</a><br>  In: <em>International Joint Conference on Artificial Intelligence <b>(IJCAI)</b></em>, 2020.<br>  (Acceptance Rate: 592/4717=12.6%) <br>[<a href="./ATL.pdf" download="ATL.pdf">Paper</a>] [<a href="https://github.com/LegenDong/ATL-Net.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/MVCNN.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Jinglin Xu, Xiangsen Zhang, <b>Wenbin Li</b>, Xinwang Liu, Junwei Han.<br> <a href="https://www.ijcai.org/Proceedings/2020/0443.pdf">Joint Multi-view 2D Convolutional Neural Networks for 3D Object Classification.</a><br> In: <em>International Joint Conference on Artificial Intelligence <b>(IJCAI)</b></em>, 2020.<br>  (Acceptance Rate: 592/4717=12.6%) <br> [<a href="./MVCNN.pdf" download="MVCNN.pdf">Paper</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/BFL.png" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Changbin Shao, Jing Huo, Lei Qi, Zhen-Hua Feng, <b>Wenbin Li</b>, Chuanqi Dong, Yang Gao.<br> <a href="https://www.ijcai.org/Proceedings/2020/0093.pdf">Biased Feature Learning for Occlusion Invariant Face Recognition.</a><br> In: <em>International Joint Conference on Artificial Intelligence <b>(IJCAI)</b></em>, 2020.<br>  (Acceptance Rate: 592/4717=12.6%) <br> [<a href="./BFL.pdf" download="BFL.pdf">Paper</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/MvNN.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman">Jinglin Xu, <b>Wenbin Li</b>, Xinwang Liu, Dingwen Zhang, Ji Liu, Junwei Han. <br><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6122">Deep Embedded Complementary and Interactive Informationfor Multi-view Classification.</a> <br> In: <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2020.<br>  (Acceptance Rate: 1591/7737=20.6%) <br> [<a href="./AAAI20a.pdf" download="AAAI20a.pdf">Paper</a>] [<a href="https://github.com/xujinglin/MvNNcor">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/LSC.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"> Xiao Liu, <b>Wenbin Li</b>, Jing Huo, Lili Yao, Yang Gao. <br> <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5927/5783">Layerwise Sparse Coding for Pruned Deep Neural Networks with Extreme Compression Ratio.</a><br> In: <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2020.<br>  (Acceptance Rate: 1591/7737=20.6%) <br> [<a href="./LSC.pdf" download="LSC.pdf">Paper</a>] </font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/DN4.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"><b>Wenbin Li</b>, Lei Wang, Jinglin Xu, Jing Huo, Yang Gao, Jiebo Luo. <br> <a  href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Revisiting_Local_Descriptor_Based_Image-To-Class_Measure_for_Few-Shot_Learning_CVPR_2019_paper.pdf"> Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning.</a> <br> In: <em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2019.<br>  (Acceptance Rate: 1299/5165=25.2%) <br> [<a href="./CVPR19.pdf" download="CVPR19.pdf">Paper</a>] [<a href="https://github.com/WenbinLee/DN4.git">Code</a>]</font></p></li></ul></td></tr></tbody></table><table class="imgtable"><tbody><tr><td><img src="./Paper/CovaMNet.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td><td align="left"><ul><li><p><font face="Times New Roman"><b>Wenbin Li<sup>#</sup></b>, Jinglin Xu<sup>#</sup>, Jing Huo, Lei Wang, Yang Gao, Jiebo Luo. <br> <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4885/4758">Distribution Consistency based Covariance Metric Networks for Few-shot Learning</a>.<br> In: <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2019. <b>[Oral]</b><br>  (Acceptance Rate: 1150/7095=16.2%) [# Equal contribution]<br> [<a href="./AAAI19.pdf" download="AAAI19.pdf">Paper</a>] [<a href="./AAAI19_slide.pdf" download="AAAI19_slide.pdf">Slides</a>] [<a href="https://github.com/WenbinLee/CovaMNet.git">Code</a>]</font></p></li></ul></td></tr></tbody></table></ol><h2><font face="Times New Roman">Datasets & Framework</font></h2><ul><table class="imgtable">  <tbody>    <tr>    <td>        <img src="./Paper/LibContinual2.png" width="250">      </td>      <td width="600"  style="vertical-align:text-top;">        <h3><font face="Times New Roman"><a href="https://github.com/RL-VIG/LibContinual.git"><b>LibContinual</b></a><br></font></h3>        <p  style=text-align:justify><font face="Times New Roman">LibContinual is a comprehensive library for continual learning, especially for continual image classification. It integrates multiple classic continual methods into a unified framework. This library is friendly for beginners in continual learning with very concise code and clear structure.</font></td>    </p>    </tr>  </tbody></table><table class="imgtable">  <tbody>    <tr>    <td>        <img src="./Paper/Libfewshot2.png" width="250">      </td>      <td width="600"  style="vertical-align:text-top;">        <h3><font face="Times New Roman"><a href="https://github.com/RL-VIG/LibFewShot.git"><b>LibFewShot</b></a><br></font></h3>        <p  style=text-align:justify><font face="Times New Roman">LibFewShot is a comprehensive library for few-shot learning, especially for few-shot image classification. It integrates multiple classic FSL methods into a unified framework, including four fine-tuning based methods, six meta-learning based methods, and eight metric-learning based methods.</font></td>    </p>    </tr>  </tbody></table><table class="imgtable">  <tbody>    <tr>	  <td>        <img src="./Paper/CaricatureJin.bmp" width="250">      </td>      <td width="600"  style="vertical-align:text-top;">        <h3><font face="Times New Roman"><a href="https://cs.nju.edu.cn/rl/WebCaricature.htm"><b>WebCaricature Dataset</b></a><br></font></h3>        <p style=text-align:justify> <font face="Times New Roman">WebCaricature dataset is developed for caricature recognition.     It is a large photograph-caricature dataset consisting of 6042 caricatures and 5974 photographs from 252 persons collected from the web.</font></td>		</p>    </tr>  </tbody></table></ul><h2><font face="Times New Roman">Teaching</font></h2><ul><li><p><font face="Times New Roma">Spring 2024: Introduction to Machine Learning, School of Intelligence Science and Technology</font></p></li><li><p><font face="Times New Roma">Fall 2024: Advances in Machine Learning, School of Computer Science</font></p></li><li><p><font face="Times New Roma">Fall 2023: Advances in Machine Learning, School of Computer Science</font></p></li><li><p><font face="Times New Roma">Fall 2024: Machine Learning, Software Institute</font></p></li><li><p><font face="Times New Roma">Fall 2023: Machine Learning, Software Institute</font></p></li><li><p><font face="Times New Roma">Spring 2023: Machine Learning, Software Institute</font></p></li><li><p><font face="Times New Roma">Spring 2022: Machine Learning, Software Institute</font></p></li><li><p><font face="Times New Roma">Spring 2021: Machine Learning, Software Institute</font></p></li><li><p><font face="Times New Roma">Spring 2020: Machine Learning, Software Institute</font></p></li></ul><h2><font face="Times New Roman">Awards & Honors</font></h2><ul><li><p><font face="华文楷体">2023 中国科协青年人才托举</font></p></li><li><p><font face="华文楷体">2023 江苏省计算机学会青年科技奖</font></p></li><li><p><font face="华文楷体">2021 江苏省双创博士</font></p></li><li><p><font face="华文楷体">2021 南京大学紫金学者</font></p></li><li><p><font face="华文楷体">2020 江苏省计算机学会优博</font></p></li><li><p><font face="华文楷体">2020 中国研究生人工智能创新大赛优秀指导教师</font></p></li></ul><h2><font face="Times New Roman">Services</font></h2><ul>    <li><p><font face="Times New Roma">    Area Chair for <a href="https://wacv2025.thecvf.com/">WACV</a> 2023, 2024, 2025 <br>    Publication Chair for <a href="https://openaccess.thecvf.com/ACCV2022">ACCV</a> 2022<br>    Reviewer for <a href="https://icml.cc/">ICML</a> 2021, 2022, 2023, 2024<br>    Reviewer for <a href="https://nips.cc/">NeurIPS</a> 2020, 2021, 2022, 2023, 2024 <br>    Reviewer for <a href="https://iclr.cc/">ICLR</a> 2022, 2023, 2024 <br>    Reviewer for <a href="https://cvpr.thecvf.com/">CVPR</a> 2020, 2021, 2022, 2023, 2024 <br>    Reviewer for <a href="https://iccv2023.thecvf.com/">ICCV</a> 2021, 2023 <br>    Reviewer for <a href="https://eccv.ecva.net/">ECCV</a> 2020, 2022, 2024 <br>    Reviewer for <a href="https://aaai.org/conference/aaai/aaai-25/">AAAI</a> 2021, 2022, 2023, 2024<br>    Reviewer for <a href="https://www.ijcai.org/proceedings/2024/170">IJCAI</a> 2023, 2024 <br>    </li><br>    <li><p><font face="Times New Roma">    Reviewer for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE Transactions on Pattern Analysis and Machine Intelligence</a> <br>    Reviewer for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IIEEE Transactions on Neural Networks and Learning Systems</a> <br>    Reviewer for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69">IEEE Transactions on Knowledge and Data Engineering</a> <br>    Reviewer for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing</a> <br>    Reviewer for <a href="https://link.springer.com/journal/11263">International Journal of Computer Vision</a> <br>    Reviewer for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">IEEE Transactions on Multimedia</a> <br>    Reviewer for <a href="https://www.jmlr.org/tmlr/index.html">Transactions on Machine Learning Research</a> <br>    Reviewer for <a href="https://www.jmlr.org/tmlr/index.html">Pattern Recognition</a> <br>    </li></ul><h2><font face="Times New Roman">Correspondence</font></h2><p>Email:<a href="mailto:liwenbin@nju.edu.cn">liwenbin@nju.edu.cn</a>; <a href="mailto:liwenbin.nju@gmail.com">liwenbin.nju@gmail.com</a><br><!-- Office:Room 1019, Computer Science Building, Xianlin Campus of Nanjing University.<br>Address:Wenbin Li, State Key Laboratory for Novel Software Technology, Nanjing University, Xianlin Campus, 163 Xianlin Avenue,<br>Qixia District, Nanjing 210023, China<br><b><font face="华文楷体">(南京市栖霞区仙林大道163号, 南京大学仙林校区, 软件新技术国家重点实验室, 210023.)</font></b></p> --><div id="footer"><!--<div id="footer-text">Page updated on 2015.12.2, via <a href="http://jemdoc.jaboc.net/">jemdoc</a>.</div>--></div></div><a href='https://clustrmaps.com/site/1bdml'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=349&t=tt&d=SXzcVa_nPEO7nuVNB3l1W3DGKNwUvnH-lNkZYlHLhKc'/></a></body></html>